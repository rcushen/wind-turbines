{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Now that we have studied the data and acquired a basic understanding of wind turbine function, we are ready to build some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "My suggestion for Engie is to build a 'digital twin' of the wind turbine. This is a digital representation of the wind turbine which we can use to simulate the behaviour of the wind turbine under a variety of conditions, allowing us to predict future behaviour, given a weather forecast, as well as identify when certain attributes of the wind turbine are not behaving as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "A digital twin can be conceived as a big 'lookup table' that maps observed inputs into expected outputs. These inputs can be individual parameters, or a set of parameters. For example, we may wish to 'look up' the expected power output of the wind turbine, given a certain wind speed (this would give the so-called *power curve*, discussed later), or instead look up the expected rotor torque given a certain wind speed, rotor speed and air pressure.\n",
    "\n",
    "We can think of this lookup table as a big joint probability distribution over the entire state space of the wind turbine. Querying the 'lookup table' is then equivalent to computing conditional probabilities over the state space. This object is highly valuable! We would expect to see probability mass concentrated around key regions in the state space; for example, high wind speeds in the afternoon should be associated with high power output, and low wind speeds at night should be associated with low power output.\n",
    "\n",
    "So! To model this joint probability distribution, we could just fit a big Gaussian mixture model to the data, in some suitably transformed space. However, this state space is very high-dimensional, and probably computationally intractable. Moreover, we know that there are important causual relationships between the different parameters, which are determined by the known physical laws that govern the function of the wind turbine. Hence a more efficient and principled approach would be to use a Bayesian network, which allows us to encode these causal relationships in a graphical model.\n",
    "\n",
    "The lookup table should also in some sense be dynamic, and able to be updated as new data is collected... that is, we should be able to incrementally evolve the JPD as new data is collected. This is a key feature of a digital twin! But we will have to do so carefully, in some sense 'skeptically'... and this can be done with a Dynamic Bayesian Network. But I won't be looking at these, for now.\n",
    "\n",
    "So: the goal is to model each wind turbine as a Bayesian network, using the historical data to learn the parameters of the network. We can then use this network to make predictions about future behaviour, given a weather forecast, and also to identify when the wind turbine is not behaving as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "There two immediate applications of this lookup table: anomaly detection and forecasting.\n",
    "\n",
    "#### Anomaly detection\n",
    "\n",
    "If we observe a state that has low probability mass, we can flag this as an anomaly. For example, high rotor torque at low wind speeds may indicate a mechanical fault. Conversely, if we know that a certain attribute should be behaving in a certain way, we can flag regions of conditionally lower probability mass. One attribute that may be of particular interest is the *power curve*, which is specified and guaranteed by the manufacturer. Per (Hau, 2005),\n",
    "\n",
    "> The power curve is a wind turbine’s official certificate of performance\n",
    "\n",
    "This sounds a lot like ship performance curves! By modelling and monitoring these power curves, we could help Engie ensure that their wind turbines are performing as expected, and identify any deviations rated performance that may be due to mechanical faults or other issues.\n",
    "\n",
    "#### Forecasting\n",
    "\n",
    "If we have a weather forecast, we can use the lookup table to predict the expected behaviour of the wind turbine, both with a point estimate and a confidence interval."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "To make this discussion a little more tangible, we can specify a rough outline of the desired API for interacting with this digital twin. Ideally, it would look something like:\n",
    "\n",
    "```python\n",
    "import DigitalTwin from toqua\n",
    "\n",
    "# Create a new wind turbine\n",
    "wt = DigitalTwin(type=\"wind_turbine\")\n",
    "\n",
    "# Train the wind turbine on some data\n",
    "wt.train(data)\n",
    "\n",
    "# Query the wind turbine\n",
    "wt.query(\n",
    "    inputs={\n",
    "        'wind_speed': 10,\n",
    "        'rotor_speed': 10,\n",
    "        'air_pressure': 1000\n",
    "    },\n",
    "    output='power'\n",
    ")\n",
    "# -> returns a probability distribution over power (and a MAP estimate)\n",
    "wt.query(\n",
    "    inputs={\n",
    "        'wind_speed': 10,\n",
    "    },\n",
    "    output='power'\n",
    ")\n",
    "# -> returns a probability distribution over power (and a MAP estimate)\n",
    "# Assess an observation for anomalous behaviour\n",
    "wt.assess(\n",
    "    inputs={\n",
    "        'wind_speed': 10,\n",
    "        'rotor_speed': 10,\n",
    "        'air_pressure': 1000,\n",
    "        'power': 1000\n",
    "    }\n",
    ")\n",
    "# -> returns a judgement on whether the observation is anomalous\n",
    "\n",
    "# Update the twin with new data\n",
    "wt.update(data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "So let's start experimenting with some Bayesian networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_sql(\"SELECT * FROM wind_turbines.observations_clean\", conn)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except oh no! I can't find any existing Python libraries that satisfactorily implement continuous Bayesian networks. There are a few that implement discrete Bayesian networks... but this seems like a very suboptimal approach. So for the sake of time, we are left to pursue some more conventional modelling approaches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Models\n",
    "\n",
    "What can we do instead... let's just see if we can at least build some robust predictive models. If this model is strongly predictive, we can still use it to identify anomalous behaviour (though in a far less elegant fashion).\n",
    "\n",
    "### A Simple Model\n",
    "\n",
    "To start with, let us construct a model of the wind turbine's active power output, as a function of the weather. We know that there is a direct causual relationship between these two observables, so we should be able to attain strong model performance.\n",
    "\n",
    "We can denote this simple model as\n",
    "\n",
    "$$ P = f(W) $$\n",
    "\n",
    "where $P \\in \\mathcal{P} \\subset \\mathbb{R}$ is the active power output, $W \\in \\mathcal{W} \\subset \\mathbb{R}^5$ is a vector of weather variables, composed of wind speed, wind direction (away from the nacelle), temperature, air pressure and humidity, and $f: \\mathcal{W} \\rightarrow \\mathbb{R}$ is a representation of the wind turbine system.\n",
    "\n",
    "However, we will immediately run into a problem here! The wind turbine system does not run in a vacuum – it is also attached to the power grid, meaning that the observed real power generated will also be a funtion of grid demand. But we do not have access to grid demand data...\n",
    "\n",
    "We can, however, use the reactive power output as a proxy (i.e. instrumental variable) for the grid demand, since:\n",
    "* Reactive power is correlated with grid demand (I think...)\n",
    "* Reactive power is not nominally correlated with the weather, and a priori, not correlated with active power.\n",
    "Our updated model is therefore\n",
    "\n",
    "$$ P = f(W, Q) $$\n",
    "\n",
    "where $Q \\in \\mathcal{Q} \\subset \\mathbb{R}$ is the reactive power output.\n",
    "\n",
    "So let's construct some simple models of this form, and see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def print_scores(model, metric, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print(f\"Train {metric.__name__}: {metric(y_train, y_pred_train):.4}\")\n",
    "    print(f\"Test {metric.__name__}: {metric(y_test, y_pred_test):.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_vars: list[str] = [\n",
    "    'ws', 'va', 'temp', 'pressure', 'humidity',\n",
    "]\n",
    "covariates: list[str] = weather_vars + ['q']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[covariates], df['p'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Model\n",
    "\n",
    "Always a sensible place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2_score: 0.8045\n",
      "Test r2_score: 0.8064\n",
      "Train mean_squared_error: 3.64e+04\n",
      "Test mean_squared_error: 3.645e+04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print_scores(model, r2_score, X_train, y_train, X_test, y_test)\n",
    "print_scores(model, mean_squared_error, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, given how simple the model is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. XGBoost\n",
    "\n",
    "Consistently SOA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2_score: 0.9901\n",
      "Test r2_score: 0.9898\n",
      "Train mean_squared_error: 1.84e+03\n",
      "Test mean_squared_error: 1.914e+03\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print_scores(model, r2_score, X_train, y_train, X_test, y_test)\n",
    "print_scores(model, mean_squared_error, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have an out-of-the box $R^2$ of 0.99, which is very good! But we can likely still do better, if we apply some more domain knowledge and use some of the additional features we have engineered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Physics-Informed Model\n",
    "\n",
    "Let's now build a more sophisticated mode, informed by the physics of wind turbines and the features that we have previously curated. We know that active power generation is closely related to rotor torque (almost perfectly)... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991439344143642"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torque_to_power_model = LinearRegression()\n",
    "torque_to_power_model.fit(df[['rm']], df['p'])\n",
    "torque_to_power_model.score(df[['rm']], df['p'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and that rotor torque is explicitly a function of wind speed and air density. In particular, we know from (Hau 2005), we know that rotor torque increases in the square of the wind velocity, and linearly in air density. So let's include these variables directly in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates: list[str] = [\n",
    "    'ws', 'ws_sq', 'ws_cb', 'ws_a',\n",
    "    'va',\n",
    "    'temp', 'temp_6h',\n",
    "    'pressure', 'humidity', 'rho',\n",
    "    'o', 'rbt'\n",
    "]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[covariates], df['p'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2_score: 0.9248\n",
      "Test r2_score: 0.9268\n",
      "Train mean_squared_error: 1.4e+04\n",
      "Test mean_squared_error: 1.378e+04\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print_scores(model, r2_score, X_train, y_train, X_test, y_test)\n",
    "print_scores(model, mean_squared_error, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see a dramatic improvement, even in just the linear model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. XGBoost\n",
    "\n",
    "Let's do a proper grid search this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END ......................max_depth=3, n_estimators=100; total time=  29.1s\n",
      "[CV] END ......................max_depth=5, n_estimators=100; total time=  48.1s\n",
      "[CV] END ......................max_depth=5, n_estimators=500; total time= 4.0min\n",
      "[CV] END ......................max_depth=3, n_estimators=200; total time=  56.5s\n",
      "[CV] END ......................max_depth=5, n_estimators=100; total time=  48.3s\n",
      "[CV] END ......................max_depth=5, n_estimators=500; total time= 3.9min\n",
      "[CV] END ......................max_depth=3, n_estimators=500; total time= 2.3min\n",
      "[CV] END ......................max_depth=5, n_estimators=500; total time= 3.7min\n",
      "[CV] END ......................max_depth=3, n_estimators=500; total time= 2.3min\n",
      "[CV] END ......................max_depth=7, n_estimators=100; total time= 1.2min\n",
      "[CV] END ......................max_depth=7, n_estimators=200; total time= 2.2min\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(xgb_reg, params, cv=3, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2_score: 0.9918\n",
      "Test r2_score: 0.9844\n",
      "Train mean_squared_error: 1.52e+03\n",
      "Test mean_squared_error: 2.916e+03\n"
     ]
    }
   ],
   "source": [
    "model = grid.best_estimator_\n",
    "\n",
    "print_scores(model, r2_score, X_train, y_train, X_test, y_test)\n",
    "print_scores(model, mean_squared_error, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we might be overfitting a little bit, but we are still obtaining high performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "So what have we learned?\n",
    "\n",
    "* Continuous Bayesian networks are hard are great but there are no good Python libraries for them!\n",
    "* We can build a simple model of wind turbine power output, using only weather data, that is highly predictive. This is a very tractable problem!\n",
    "* This model can be used both for forecasting and anomaly detection (though I haven't demonstrated this here).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "* Build a library for continuous Bayesian networks!\n",
    "* Get a better measure of grid demand, such that we can isolate the effect of the weather on active power output.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "* https://www.sciencedirect.com/science/article/abs/pii/S0951832020305548\n",
    "* https://www.youtube.com/watch?v=ZuSx0pYAZ_I"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
